shef.mt.copyright = (c) University of Wolverhampton, 2011

! General configuration parameters:

logger.folder 							= log
logger.on 

features.default 						= all
sourceLang.default 						= chinese
targetLang.default 						= english

output								= output/test
input 								= input/test
resourcesPath 							= ./lang_resources
featureConfig.bb 						= config/features/features_blackbox_79.xml
featureConfig.gb 						= config/features/features_glassbox.xml
featureConfig.all 						= config/features/features_all_cleaned.xml

! Resources for chinese:
! please use utf8 version of the tree-tagger scripts AND 
! utf8-tokenize.perl version available in tree tagger scripts under "cmd" directory.
! TOKENIZER=${CMD}/tokenize.pl should be changed with TOKENIZER=${CMD}/utf8-tokenize.perl in cmd/tree-tagger-xxx script

chinese.postagger 						= shef.mt.tools.PosTreeTagger
chinese.postagger.exePath					= /export/tools/tree-tagger/tree-tagger-chinese
chinese.ngramScript.path 					= /export/tools/srilm/bin/i686-m64/ngram
chinese.ngramCountScript.path 					= /export/tools/srilm/bin/i686-m64/ngram-count
chinese.ngram.outputFolder 					= chinese
chinese.ngram.outputFileExt 					= ngram
chinese.poslm 							= ./lang_resources/chinese/pos_lm.ch
chinese.LMPosPerplexity.outputFileExt 				= pos.ppl
chinese.corpus							= ./lang_resources/chinese/sample_corpus.ch
chinese.lm							= ./lang_resources/chinese/chinese_lm.lm
chinese.tokenizer 						= ./lang_resources/tokenizer/tokenizer.perl
chinese.lowercase 						= ./lang_resources/tokenizer/lowercase.perl
chinese.truecase 						= ./lang_resources/tokenizer/truecase.perl
chinese.truecase.model 						= ./lang_resources/chinese/truecase-model.es
chinese.preprocess						= 0
target.wordgraph 						= ./lang_resources/chinese/wordgraph.es
target.stopwordlist						= ./lang_resources/chinese/chinese.stopwords.txt
target.ngramValueList						= ./lang_resources/chinese/ngram-target
!chinese.ngram                                                   = ./lang_resources/chinese/chinese_ngram.ngram.clean

! Resources for english:

english.ngram                                                   = ./lang_resources/english/english_ngram.ngram.clean
english.tokenizer                                               = ./lang_resources/tokenizer/tokenizer.perl
english.lowercase                                               = ./lang_resources/tokenizer/lowercase.perl
english.corpus                                                  = ./lang_resources/english/sample_corpus.en
english.lm							= ./lang_resources/english/english_lm.lm
english.truecase                                                = ./lang_resources/tokenizer/truecase.perl
english.truecase.model                                  	= ./lang_resources/english/truecase-model.en
english.postagger 						= shef.mt.tools.PosTreeTagger
english.postagger.exePath					= /export/tools/tree-tagger/cmd/tree-tagger-english-utf8
english.ngramCountScript.path 					= /export/tools/srilm/bin/i686-m64/ngram-count
english.ngram.outputFolder 					= english
english.ngram.outputFileExt 					= ngram
english.preprocess						= 0
english.poslm							= ./lang_resources/english/pos_lm.en

! Resources for the language pair:

alignments.file                                                 = ./lang_resources/alignments/alignments.word-level.out
pair.chineseenglish.giza.path                   		= ./lang_resources/giza/lex.c2e
sourcetarget.postagger.scripts                        		= /export/tools/tree-tagger/
source.stopwordlist						= ./lang_resources/english/englishstopwords.txt

! Language model resources:

tools.ngram.path 						= /export/tools/srilm/bin/i686-m64/
tools.irstlm.path 						= /export/tools/irstlm/
tools.ngram.output.ext 						= .ppl
ngramsize       						= 3

TM 								= 0 
chinese.topic.distribution      				= ./lang_resources/wmt12.test.es
english.topic.distribution      				= ./lang_resources/wmt12.test.en

! Berkeley features resources:

featureConfig.parser 						= config/features/features_parser.xml
mode 								= parser
BP 								= 0
chinese.bparser.grammar                         		= ./lang_resources/spa_ancora.gr
chinese.bparser.kbest                                   	= 600
english.bparser.grammar                         		= ./lang_resources/eng_sm6.gr
chinese.bparser.kbest                                   	= 600

! Resources for Global lexicon:

GL 								= 0
pair.englishchinese.glmodel.path                        	= ./lang_resources/glmodel
pair.englishchinese.glmodel.minweight                   	= 0.5

! Resources for triggers:
! ======================
! TR must be set to 1 if using triggers features
TR 								= 0
! a trigger file is made up of one line for each couple
! a couple is a word or phrase couple
! a phrase is made up of the concatenation of word with the phrase.separator
! a line contains : word1(or phrase) word2(or phrase) nbw1 nbw2 nbw1w1 mi probmi
! where :
! nbw1 is the number of sentences where word1 occurs
! nbw2 is the number of sentences where word2 occurs
! nbw1w2 is the number of couple of soource/target sentences where word1 occurs in source sentence and word2 occurs in target sentence
! mi is log10((N*nbw1w2)/(nbw1*nbw2)), where N is the number of lines in the parallel training corpus
! pmi is (nbw1w2/N)*mi
!
! For intra lingual triggers, word1 and word2 are in the same language
! For inter lingual triggers, word1 and word2 are in different languages
!
!
! file containing source triggers for example president --> Obama
source.intra.triggers.file   					= Data/triggers_en_en.20
! when loaded, only the nb.max.triggers.source.intra best triggers are kept for each word
nb.max.triggers.source.intra 					= 5
! file containing target triggers for example presidente --> Obama
target.intra.triggers.file   					= Data/triggers_es_es.20
! when loaded, only the nb.max.triggers.target.intra best triggers are kept for each word
nb.max.triggers.target.intra 					= 5
! file containing source-target triggers for example speak --> hablar
source.target.inter.triggers.file 				= Data/triggers_en_es.20
! when loaded, only the nb.max.triggers.source.target.inter best triggers are kept for each word
nb.max.triggers.source.target.inter 				= 5
! phrases are possible. A phrase is actually used as a word. The phrase.separator allows to concatenate words sequence into one whole word
phrase.separator 						= __

! Gb parameters:

nbestSize 							= 1000
